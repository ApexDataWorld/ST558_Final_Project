---
title: "Modeling"
format: html
editor: visual
---

Reading Data

Import the libraries

```{r}
library(tidyverse)
library(ggplot2)
library(tidymodels)
library(rpart.plot)
library(future)
```

```{r}
plan(multisession, workers = 4)  
```

```{r}
health_data <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv")
head(health_data)
```

The modeling script reads in the same diabetes dataset used in the EDA. The first few rows confirm that the predictors and outcome look the same as before, so the modeling work is directly aligned with our earlier exploration.

```{r}
health_data <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv")

health_data <- health_data |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary,
      levels = c(0, 1),
      labels = c("NoDiabetes", "Diabetes")),
    HighBP = factor(HighBP, levels = c(0,1), labels = c("No", "Yes")),
    HighChol = factor(HighChol, levels = c(0,1), labels = c("No", "Yes")),
    CholCheck = factor(CholCheck, levels = c(0,1), labels = c("No", "Yes")),
    Smoker = factor(Smoker, levels = c(0,1), labels = c("No", "Yes")),
    Stroke = factor(Stroke, levels = c(0,1), labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1),
    labels = c("No","Yes")),
    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c("No", "Yes")),
    Fruits = factor(Fruits, levels = c(0,1), labels = c("No", "Yes")),
    Veggies = factor(Veggies, levels = c(0,1), labels = c("No", "Yes")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1),
      labels = c("No", "Yes")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0,1), labels = c("No", "Yes")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0,1), labels = c("No", "Yes")),
    GenHlth = factor(GenHlth, levels = c(1:5),
      labels = c("Excellent", "Very Good", "Good", "Fair", "Poor")),
    DiffWalk = factor(DiffWalk, levels = c(0,1), labels = c("No", "Yes")),
    Sex = factor(Sex, levels = c(0,1), labels = c("Female", "Male")),
    Age = factor(Age, levels = c(1:13),
      labels = c("18-24", "25-29", "30-34", "35-39", "40-44",
        "45-49", "50-54", "55-59", "60-64",
        "65-69", "70-74", "75-79", "80+")),
    Education = factor(Education, levels = c(1:6),
      labels = c("KG or No School", "Elementary", "Middle school",
        "High school", "College","Professional Degree")),
    Income = factor(Income, levels = c(1:8),
      labels = c("<10K", "$10k-$15k", "$15k-$20k", "$20k-$25k",
        "$25k-$35k", "$35k-$50k", "$50k-$75k", "$75k+"))
  )

print(health_data)

```

All the categorical predictors are converted from 0/1 codes into labeled factors again. This ensures that the modeling functions treat them as categorical variables and makes model summaries easier to interpret.

Make a model for diabetes data. Split the data into 70(training) and 30(testing) percent.

On the training set, create a 5 fold CV split

```{r}
set.seed(37)
diabetes_split <- initial_split(health_data, prop = 0.7, strata = Diabetes_binary)
diabetes_train <- training(diabetes_split)
diabetes_test  <- testing(diabetes_split)

diabetes_CV_folds <- vfold_cv(diabetes_train, v = 5)
#print(diabetes_CV_folds)

```

The data is split into 70% training and 30% testing, stratified by diabetes status. This keeps the diabetes vs. no-diabetes proportions similar in both sets and gives us a fair way to evaluate out of sample performance later.

```{r}
diabetes_CV_folds
```

We set up 5 fold cross validation on the training data. This means the training set is reused efficiently: in each fold, part of the data is used to fit the model and the rest to estimate performance. This helps us tune model hyperparameters without touching the test set.

### Fitting Logistic Regression Models

First of all i have to set up our recipes for the data, standardizing the BMI numeric variable

for the 1st recipe:

Model 1: BMI and Smoker

Model 2: BMI, Smoker, HighBP, HearthDiseaserorAttack, PhysActivity, Sex

Model 3: All the predictors

```         
```

```{r}
LR2_recipe <- recipe(
  Diabetes_binary ~ BMI + Smoker + HighBP + HeartDiseaseorAttack + PhysActivity + Sex,
    data = health_data
  ) |>
  step_normalize(all_numeric(), -Diabetes_binary)
```

The recipe focuses on a smaller set of predictors: BMI, Smoker, HighBP, HeartDiseaseorAttack, PhysActivity, and Sex. BMI is standardized, while the categorical variables are kept as factors. These predictors are clinically reasonable risk factors for diabetes and keep the model simpler than using all 21 predictors.

```{r}


```

```{r}
LR2_recipe |>
  prep(diabetes_train) |>
  bake(diabetes_train) |>
  colnames()
```

```{r}
# LR3_recipe |>
#   prep(diabetes_train) |>
#   bake(diabetes_train) |>
#   colnames()
```

Classification Tree

Decision Tree

Model Specification

```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Create our Workflow

```{r}
tree_wkf <- workflow() |>
  add_recipe(LR2_recipe) |>
  add_model(tree_mod)
```

We set up a classification tree model where tree depth and cost complexity are tunable parameters. The workflow ties together the pre-processing (recipe) and the tree model so they can be tuned and evaluated in one consistent pipeline.

Fit the model with tune_grid() and grid_regular()

```{r}
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)
```

```{r}
tree_fits <- tree_wkf |>
  tune_grid(resamples = diabetes_CV_folds,
            grid = tree_grid,
            metrics = metric_set(accuracy, mn_log_loss)
  )

tree_fits
```

The tuning grid explores different combinations of tree depth and cost complexity. Cross validation finds which combination gives the best balance between accuracy and log loss. This helps avoid an overly shallow (underfit) or overly deep (overfit) tree.

Let see which is the best accuracy

```{r}
best_tree <- select_best(tree_fits, metric = "mn_log_loss")
print(best_tree)
```

The best tree has a depth of about 8 and a very small cost complexity value. This suggests the model benefits from allowing a reasonably deep tree, but still with a small penalty to prevent extreme overfitting.

Finalize the workflow and fit the model

```{r}
final_tree <- finalize_workflow(tree_wkf, best_tree) |>
  fit(data = diabetes_train)
print(final_tree)
```

The tree splits first on high blood pressure and BMI, and later on variables like heart disease, physical activity, and smoking. This indicates that high blood pressure and higher BMI are key drivers in separating people with and without diabetes in this dataset.

Evaluate the model on the test set

\# TRY

```{r}
tree_pred <- predict(final_tree, diabetes_test, type = "prob") |>
  bind_cols(diabetes_test |> select(Diabetes_binary))

print(tree_pred)
```

log loss on test data

```{r}

# Log-loss
tree_log_loss <- mn_log_loss(
  tree_pred,
  truth = Diabetes_binary,
  .pred_Diabetes
) %>% pull(.estimate)
print(tree_log_loss)

```

The decision tree achieves a log loss of about 2.1 on the test set. This is a relatively high log loss, which means the probability predictions are not very “confident” or well calibrated, especially for the minority (diabetes) class.

Convert to class labels with 0.5 threshold and compute accuracy:

```{r}
# Class labels using 0.5 threshold on Diabetes prob
tree_final_predictions <- tree_pred |>
  mutate(
    .pred_class = if_else(
      .pred_Diabetes > 0.5,
      "Diabetes",
      "NoDiabetes"
    ),
    .pred_class = factor(.pred_class, levels = levels(Diabetes_binary))
  )

tree_final_metrics <- yardstick::metrics(
  tree_final_predictions,
  truth   = Diabetes_binary,
  estimate = .pred_class
)

print(tree_final_metrics)

```

Using a 0.5 threshold, the decision tree reaches roughly 86% accuracy on the test data, but the kappa value is quite low. This tells us that most of the accuracy is coming from correctly predicting the majority class (NoDiabetes), and the model is not doing a great job distinguishing diabetes cases from non-cases.

# Try

## Fitting Random Forest

```{r}
rf_spec <- rand_forest(mtry = tune(),
                       trees = 1000, 
                       min_n = tune()) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")
```

Create the workflow

```{r}
rf_wkf <- workflow() |>
  add_recipe(LR2_recipe) |>
  add_model(rf_spec)
print(rf_wkf)
```

The random forest model uses the same set of predictors and recipe as the tree but fits many trees and aggregates them. We tune `mtry` (number of predictors tried at each split) and `min_n` (minimum node size), with 1000 trees and impurity-based variable importance.

fit model with tune_grid() and tune_regular()

```{r}
rf_fit <- rf_wkf |>
  tune_grid(resamples = diabetes_CV_folds,
            grid = 12,
            metrics = metric_set(accuracy, mn_log_loss))
```

Cross-validation suggests that a relatively small `mtry` (around 3) and a moderate `min_n` (e.g., 22) work best. Different tuning combinations give very similar performance, which indicates the random forest is fairly stable and robust to these settings.Let's see which tuning parameters is best

```{r}
rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")
print(rf_best_params)
```

Finalize the workflow with the best parameters

```{r}
rf_final_wkf <- rf_wkf |>
  finalize_workflow(rf_best_params)
```

Fit the finalize model and evaluate on the test set

```{r}
rf_final_fit <- rf_final_wkf |>
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))
print(rf_final_fit)
```

With the best tuning parameters, the random forest achieves about 86.3% accuracy on the test set, which is similar to the decision tree. However, its log-loss on the test set is much lower (around 0.34), which means its probability predictions are much better calibrated and more reliable.

```{r}

```

```{r}
final_rf_full <- rf_wkf |>
  finalize_workflow(rf_best_params) |>
  fit(data = health_data)

saveRDS(final_rf_full, file = "data/final_rf_full.rds")
```

-   Decision Tree Test set performance

```{r}

```

```{r}

```

```{r}

# Extract the finalized RF workflow from last_fit()
rf_workflow_extracted <- extract_workflow(rf_final_fit)

# Predict probabilities on the test set
rf_pred <- predict(rf_workflow_extracted, diabetes_test, type = "prob") |>
  bind_cols(diabetes_test %>% select(Diabetes_binary))

# Log-loss for Random Forest (test set)
rf_log_loss <- mn_log_loss(
  rf_pred,
  truth = Diabetes_binary,
  .pred_Diabetes
) %>% pull(.estimate)

# Convert RF predicted probabilities to class labels for accuracy evaluation
rf_pred <- rf_pred |>
  mutate(
    .pred_class = if_else(
      .pred_Diabetes > 0.5,
      "Diabetes",
      "NoDiabetes"
    ),
    .pred_class = factor(.pred_class, levels = levels(Diabetes_binary))
  )

# Evaluate RF metrics
rf_metrics <- rf_pred |>
  yardstick::metrics(truth = Diabetes_binary, estimate = .pred_class)

print(rf_metrics)

```

The random forest has accuracy comparable to the decision tree but a noticeably better log-loss and slightly better kappa. This indicates that, while both models correctly classify a similar fraction of cases, the random forest is more informative and distinguishes the two classes more effectively.

```{r}
# Compare models}
# model_comparison <- tibble(
#   Model = c("Decision Tree", "Random Forest"),
#   Log_Loss = c(tree_log_loss, rf_log_loss)
# )
# 
# print(model_comparison)

print(tree_log_loss)
print(rf_log_loss)
```

-   Random Forest Test Set performance

```{r}
rf_final_metrics <- rf_final_fit |>
  collect_metrics()

print(rf_final_metrics)
```

Across all cross validation folds and tuning combinations, the best random forest models cluster around a mean log loss of about 0.345 with very small standard errors. This consistency suggests the random forest’s performance is stable and not heavily dependent on a single random split.

Collect and view matrics

```{r}
 rf_fit |>
   collect_metrics() |>
   filter(.metric == "mn_log_loss") |>
   arrange(mean)

print(rf_metrics)
```

### Compare Performance

```{r}
# Combine Metrics into a DataFrame for Comparison
model_comparison <- tibble(
  Model = c("Decision Tree", "Random Forest"),
  Accuracy = c(
    tree_final_metrics |> filter(.metric == "accuracy") |> pull(.estimate),
    rf_metrics         |> filter(.metric == "accuracy") |> pull(.estimate)
  ),
  Log_Loss = c(
    tree_log_loss,
    rf_log_loss
  )
)

print(model_comparison)

# Declare winner based on log-loss
if (tree_log_loss < rf_log_loss) {
  winner <- "Decision Tree"
} else {
  winner <- "Random Forest"
}

print(winner)


```

Both models have similar test accuracy (\~0.86), but the random forest has far better log-loss than the decision tree. If we care about good probability estimates (for example, risk scores), the random forest is clearly the better choice, so it becomes our preferred model.

```{r}
# Declare winner based on log-loss
model_comparison <- tibble(
  Model = c("Decision Tree", "Random Forest"),
  Accuracy = c(
    tree_final_metrics |> filter(.metric == "accuracy") |> pull(.estimate),
    rf_metrics         |> filter(.metric == "accuracy")         |> pull(.estimate)
  ),
  Log_Loss = c(
    tree_log_loss,
    rf_log_loss
  )
)


if (tree_log_loss < rf_log_loss) {
  winner <- "Decision Tree"
} else {
  winner <- "Random Forest"
}
print(winner)

```

Based on log loss performance, we select the random forest as the final model. It balances strong classification accuracy with much better calibrated probabilities compared to the single decision tree.

Plot the variable importance

```{r}

# Extract the underlying ranger engine from FULL model
rf_engine <- extract_fit_engine(final_rf_full)  

# Get importance as a named numeric vector
vi <- rf_engine$variable.importance
vi 

vi_df <- tibble(
  term  = names(vi),
  value = as.numeric(vi)
)

```

The variable importance scores show that high blood pressure and BMI are the most influential predictors for the random forest. History of heart disease or heart attack and physical activity also contribute meaningfully, while smoking and sex have smaller, but still non-zero, importance.

```{r}
colnames(vi_df)
```

```{r}
vi_df |>
  arrange(desc(value)) |>
  mutate(term = factor(term, levels = rev(term))) |>
  ggplot(aes(x = term, y = value)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  labs(
    title = "Variable Importance (Random Forest)",
    x     = "Predictor",
    y     = "Importance"
  ) +
  theme_minimal()

```

The bar plot makes it clear that `HighBP`, `BMI`, and `HeartDiseaseorAttack` stand out as the top predictors in the model. This is consistent with medical knowledge that higher blood pressure, higher body weight, and prior heart conditions are closely linked to diabetes risk.

### Save the file for api

```{r}
# save the training data set
 saveRDS(diabetes_train, file = "data/diabetes_train.rds")
 
# save the random forest tree model
 saveRDS(rf_final_fit, file = "data/final_rf_model.rds")
 
# save the comparison metrics
 saveRDS(model_comparison, file = "data/model_comparison.rds")
```

The final training data, the tuned random forest model, and the summary of model performance are saved to disk. These saved files will be used later when we build the API, so we can load the model and make predictions without having to retrain it.

```{r}

```

Link to Index Page

[Click here for the Index Page](index.html)
